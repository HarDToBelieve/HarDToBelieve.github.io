<dec f='webkit/Source/ThirdParty/libwebrtc/Source/webrtc/modules/audio_processing/include/audio_processing.h' l='652' type='int webrtc::AudioProcessing::set_stream_delay_ms(int delay)'/>
<doc f='webkit/Source/ThirdParty/libwebrtc/Source/webrtc/modules/audio_processing/include/audio_processing.h' l='639'>// This must be called if and only if echo processing is enabled.
  //
  // Sets the |delay| in ms between ProcessReverseStream() receiving a far-end
  // frame and ProcessStream() receiving a near-end frame containing the
  // corresponding echo. On the client-side this can be expressed as
  //   delay = (t_render - t_analyze) + (t_process - t_capture)
  // where,
  //   - t_analyze is the time a frame is passed to ProcessReverseStream() and
  //     t_render is the time the first sample of the same frame is rendered by
  //     the audio hardware.
  //   - t_capture is the time the first sample of a frame is captured by the
  //     audio hardware and t_process is the time the same frame is passed to
  //     ProcessStream().</doc>
<use f='webkit/Source/ThirdParty/libwebrtc/Source/webrtc/audio/audio_transport_impl.cc' l='55' u='c' c='_ZN6webrtc12_GLOBAL__N_119ProcessCaptureFrameEjbbPNS_15AudioProcessingEPNS_10AudioFrameE'/>
<ovr f='webkit/Source/ThirdParty/libwebrtc/Source/webrtc/modules/audio_processing/audio_processing_impl.cc' l='1565' c='_ZN6webrtc19AudioProcessingImpl19set_stream_delay_msEi'/>
<dec f='webkit/WebKitBuild/Debug/usr/local/include/webrtc/modules/audio_processing/include/audio_processing.h' l='652' type='int webrtc::AudioProcessing::set_stream_delay_ms(int delay)'/>
<doc f='webkit/WebKitBuild/Debug/usr/local/include/webrtc/modules/audio_processing/include/audio_processing.h' l='639'>// This must be called if and only if echo processing is enabled.
  //
  // Sets the |delay| in ms between ProcessReverseStream() receiving a far-end
  // frame and ProcessStream() receiving a near-end frame containing the
  // corresponding echo. On the client-side this can be expressed as
  //   delay = (t_render - t_analyze) + (t_process - t_capture)
  // where,
  //   - t_analyze is the time a frame is passed to ProcessReverseStream() and
  //     t_render is the time the first sample of the same frame is rendered by
  //     the audio hardware.
  //   - t_capture is the time the first sample of a frame is captured by the
  //     audio hardware and t_process is the time the same frame is passed to
  //     ProcessStream().</doc>
